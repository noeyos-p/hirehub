# main.py â€” FULL Combined Version
# ë‰´ìŠ¤ / moderation / review / match-one / embed / models / health ì „ë¶€ í¬í•¨
# + ì„ë² ë”© ê¸°ë°˜ Spring ë§¤ì¹­ ì§€ì›
# + 10ë§Œ job_posts í™•ì¥ì„ ìœ„í•œ ì†ë„ ìµœì í™” (ì„ë² ë”© ìºì‹œ-friendly)

import os, sys, re, json
from datetime import datetime, timedelta
from typing import List, Optional

sys.stdout.reconfigure(encoding='utf-8')

# ===== ENV ì ìš© =====
from dotenv import dotenv_values, load_dotenv
env = dotenv_values(".env")
for k, v in env.items():
    if v:
        os.environ[k] = v
load_dotenv()

import requests
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import google.generativeai as genai
import time
from functools import wraps
from typing_extensions import TypedDict
from collections import deque
from fastapi.responses import StreamingResponse
import asyncio

# ===== Gemini ì„¤ì • =====
API_KEY = os.getenv("GEMINI_API_KEY", "")
genai.configure(api_key=API_KEY)

print("âœ… FastAPI Loaded â€” ALL FEATURES READY")


# ------------------------------------------------------
# ê³µí†µ ìœ í‹¸
# ------------------------------------------------------

def safe_json(s: str):
    """JSON íŒŒì‹± ì•ˆì „ ë²„ì „"""
    try:
        return json.loads(s)
    except:
        m = re.search(r"\{.*\}", s, re.S)
        if not m:
            return {}
        try:
            return json.loads(m.group(0))
        except:
            return {}


def safe_json_v2(s: str):
    """
    Gemini API ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ (ê°œì„  ë²„ì „)
    """
    if not s:
        return {}

    # 1. Markdown ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ```)
    s = re.sub(r'```json\s*', '', s)
    s = re.sub(r'```\s*$', '', s)
    s = re.sub(r'^```\s*', '', s)

    # 2. ì•ë’¤ ê³µë°± ì œê±°
    s = s.strip()

    # 3. ì§ì ‘ íŒŒì‹± ì‹œë„
    try:
        return json.loads(s)
    except:
        pass

    # 4. JSON ê°ì²´ë§Œ ì¶”ì¶œ ì‹œë„
    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', s, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass

    # 5. ìµœí›„ì˜ ìˆ˜ë‹¨: scoreì™€ reasonë§Œ ì¶”ì¶œ
    try:
        score_match = re.search(r'"score"\s*:\s*(\d+)', s)
        reason_match = re.search(r'"reason"\s*:\s*"([^"]*)"', s)

        if score_match:
            score = int(score_match.group(1))
            reason = reason_match.group(1) if reason_match else "ë¶„ì„ ì™„ë£Œ"
            return {"score": score, "reason": reason}
    except:
        pass

    return {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}


# â­ ì´ í•¨ìˆ˜ê°€ ë¨¼ì € ì •ì˜ë˜ì–´ì•¼ í•¨!
def call_llm(model, system, prompt, max_tokens=512, temperature=0.3):
    """LLM í˜¸ì¶œ (ëª¨ë“  ê¸°ëŠ¥ ê³µí†µ ì‚¬ìš©)"""
    try:
        m = genai.GenerativeModel(
            model_name=model,
            system_instruction=system
        )
        r = m.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
                top_k=1,
                top_p=0.95,
                candidate_count=1,
            )
        )

        cand = r.candidates[0] if r.candidates else None
        if cand and cand.content.parts:
            return "\n".join([p.text for p in cand.content.parts if getattr(p, "text", None)])
    except Exception as e:
        print(f"[LLM ERROR] {e}")
    return None


def call_llm_with_json(model, system, prompt, max_tokens=512, temperature=0.3):
    """JSON ì‘ë‹µì„ ê°•ì œí•˜ëŠ” LLM í˜¸ì¶œ"""
    try:
        m = genai.GenerativeModel(
            model_name=model,
            system_instruction=system,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
                top_k=1,
                top_p=0.95,
                candidate_count=1,
                response_mime_type="application/json",  # â­ JSON ê°•ì œ
            )
        )

        r = m.generate_content(prompt)

        cand = r.candidates[0] if r.candidates else None
        if cand and cand.content.parts:
            text = "\n".join([p.text for p in cand.content.parts if getattr(p, "text", None)])
            return text
    except Exception as e:
        print(f"[LLM JSON ERROR] {e}")

    return None


def generate_text(system, prompt, max_tokens=512, temperature=0.3):
    """2ë‹¨ê³„ ëª¨ë¸ í´ë°± í¬í•¨"""
    primary = "models/gemini-2.5-flash"
    fallback = "models/gemini-flash-latest"

    out = call_llm(primary, system, prompt, max_tokens, temperature)
    if out:
        return out

    out = call_llm(fallback, system, prompt, max_tokens, temperature)
    return out or "âš ï¸ ëª¨ë¸ ë¬´ì‘ë‹µ"


def generate_json(system, user_prompt, schema_hint, max_tokens=512, temperature=0.2):
    """JSONë§Œ ê°•ì œë¡œ ë½‘ì•„ì˜¤ëŠ” í—¬í¼"""
    prompt = (
        f"{user_prompt}\n\në°˜ë“œì‹œ JSONë§Œ ì¶œë ¥.\nìŠ¤í‚¤ë§ˆ: {schema_hint}"
    )
    out = generate_text(system, prompt, max_tokens, temperature)
    data = safe_json(out)
    if data:
        return data
    return safe_json(out)


# ------------------------------------------------------
# FastAPI ì„¤ì •
# ------------------------------------------------------

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# ------------------------------------------------------
# Schemas
# ------------------------------------------------------

class NewsItem(BaseModel):
    title: str
    link: str
    description: Optional[str] = None
    pubDate: Optional[str] = None
    press: Optional[str] = None

class DigestRequest(BaseModel):
    query: str = "ì±„ìš© OR ê³µì±„"
    days: int = 3
    limit: int = 20
    style: str = "bullet"

class ReviewRequest(BaseModel):
    content: str

class MatchOneRequest(BaseModel):
    resume: Optional[str] = ""
    job: Optional[str] = ""

class EmbedRequest(BaseModel):
    text: str


class RateLimiter:
    """ë¬´ë£Œ í‹°ì–´ Rate Limit ê´€ë¦¬"""

    def __init__(self, rpm=15, rpd=1500):
        self.rpm = rpm  # ë¶„ë‹¹ ìš”ì²­
        self.rpd = rpd  # ì¼ì¼ ìš”ì²­
        self.minute_queue = deque()
        self.daily_count = 0
        self.daily_reset = datetime.now() + timedelta(days=1)

    def check_and_wait(self):
        """Rate limit ì²´í¬ ë° ëŒ€ê¸°"""
        now = datetime.now()

        # ì¼ì¼ ì œí•œ ë¦¬ì…‹
        if now >= self.daily_reset:
            self.daily_count = 0
            self.daily_reset = now + timedelta(days=1)

        # ì¼ì¼ ì œí•œ ì²´í¬
        if self.daily_count >= self.rpd:
            raise Exception(f"ì¼ì¼ ìš”ì²­ ì œí•œ ì´ˆê³¼ ({self.rpd}íšŒ)")

        # 1ë¶„ ì´ì „ ìš”ì²­ ì œê±°
        one_minute_ago = now - timedelta(minutes=1)
        while self.minute_queue and self.minute_queue[0] < one_minute_ago:
            self.minute_queue.popleft()

        # ë¶„ë‹¹ ì œí•œ ì²´í¬ ë° ëŒ€ê¸°
        if len(self.minute_queue) >= self.rpm:
            # ê°€ì¥ ì˜¤ë˜ëœ ìš”ì²­ì´ 1ë¶„ ê²½ê³¼í•  ë•Œê¹Œì§€ ëŒ€ê¸°
            wait_until = self.minute_queue[0] + timedelta(minutes=1)
            wait_seconds = (wait_until - now).total_seconds()
            if wait_seconds > 0:
                print(f"â³ Rate limit ëŒ€ê¸°: {wait_seconds:.1f}ì´ˆ")
                time.sleep(wait_seconds + 0.1)

        # ìš”ì²­ ê¸°ë¡
        self.minute_queue.append(now)
        self.daily_count += 1

        return True


# ì „ì—­ Rate Limiter
gemini_limiter = RateLimiter(rpm=15, rpd=1500)

# ------------------------------------------------------
# NAVER ë‰´ìŠ¤ íŒŒíŠ¸
# ------------------------------------------------------

def naver_news_search(query, start=1, display=20, sort="date"):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {"query": query, "display": display, "start": start, "sort": sort}
    r = requests.get("https://openapi.naver.com/v1/search/news.json",
                     headers=headers, params=params, timeout=10)
    r.raise_for_status()
    return r.json()



@app.post("/news/fetch", response_model=List[NewsItem])
def fetch_news(req: DigestRequest):
    items = []
    remaining = req.limit
    start = 1

    while remaining > 0 and start <= 1000:
        size = min(20, remaining)
        data = naver_news_search(req.query, start, size)
        raw = data.get("items", [])
        if not raw:
            break

        for it in raw:
            items.append(
                NewsItem(
                    title=it["title"].replace("<b>", "").replace("</b>", ""),
                    link=it["link"],
                    description=it.get("description", "").replace("<b>", "").replace("</b>", ""),
                    pubDate=it.get("pubDate"),
                    press=it.get("originallink"),
                )
            )
        remaining -= len(raw)
        start += size

    # ë‚ ì§œ í•„í„°
    if req.days > 0:
        cutoff = datetime.utcnow() - timedelta(days=req.days + 0.5)

        def in_range(n):
            try:
                dt = datetime.strptime(n.pubDate, "%a, %d %b %Y %H:%M:%S %z")
                return dt.timestamp() >= cutoff.timestamp()
            except:
                return False

        items = [n for n in items if in_range(n)]

    return items[:req.limit]


@app.post("/news/digest")
def news_digest(req: DigestRequest):
    queries = [
        "ì±„ìš© OR ê³µì±„ OR ë…¸ë™ì‹œì¥",
        "IT OR ê¸°ìˆ  OR ê°œë°œì OR AI",
        "ê²½ì œ OR ì‚°ì—… OR ì‹œì¥"
    ]

    items = []
    for q in queries:
        items = fetch_news(DigestRequest(query=q, days=req.days, limit=req.limit))
        if items:
            break

    if not items:
        return {"title": "ë‰´ìŠ¤ ì—†ìŒ", "content": "", "tags": [], "sources": []}

    titles = "\n".join(f"- {n.title}" for n in items)

    out = generate_text(
        "ë„ˆëŠ” ë‰´ìŠ¤ ì „ë¬¸ í¸ì§‘ìë‹¤.",
        f"""ë‹¤ìŒ ì œëª©ì„ {req.style} ìŠ¤íƒ€ì¼ë¡œ ìš”ì•½í•˜ë¼:
{titles}
""",
        max_tokens=500,
        temperature=0.25
    )

    uid = datetime.now().strftime("%Y%m%d%H%M%S")
    return {
        "title": f"AI ë‰´ìŠ¤ ìš”ì•½ ({uid})",
        "content": out.strip(),
        "tags": ["ë‰´ìŠ¤", "ìš”ì•½"],
        "sources": items,
    }


# ------------------------------------------------------
# Moderation
# ------------------------------------------------------

KOREAN_BAD_RE = re.compile(r"(ì”¨ë°œ|ì‹œë°œ|ã……\s*ã…‚|ê°œ\s*ìƒˆë¼|ë³‘ì‹ |ì¢†|ì§€ë„|ì”¹)", re.I)
SPAM_RE = [r"(http|https)://", r"ì˜¤í”ˆì±„íŒ…", r"ìˆ˜ìµ\s?ë³´ì¥", r"ìƒë‹´\s?ë¬¸ì˜"]

@app.post("/ai/moderate")
def moderate(req: dict):
    text = (req or {}).get("content", "") or ""

    if KOREAN_BAD_RE.search(text) or any(re.search(p, text, re.I) for p in SPAM_RE):
        return {"approve": False, "reason": "ë£° ê¸°ë°˜ ì°¨ë‹¨", "categories": {"rule": 1.0}}

    data = generate_json(
        "ë„ˆëŠ” ì½˜í…ì¸  ì•ˆì „ ì‹¬ì‚¬ê´€ì´ë‹¤.",
        f"ì•„ë˜ ê¸€ ìœ„í—˜ë„ ë¶„ì„:\n{text}",
        schema_hint='{"approve": bool, "categories": {}, "reason": ""}'
    )

    cats = data.get("categories", {})
    risk_values = [float(v) for v in cats.values()] if cats else []
    max_risk = max(risk_values) if risk_values else 0

    approve = data.get("approve", True)
    if max_risk >= 0.5:
        approve = False

    return {"approve": approve, "reason": data.get("reason", ""), "categories": cats}


# ------------------------------------------------------
# Review
# ------------------------------------------------------

@app.post("/ai/review")
def review_resume(req: ReviewRequest):
    """
    ì´ë ¥ì„œ/ìì†Œì„œ ì²¨ì‚­ ê¸°ëŠ¥ (ê°œì„  ë²„ì „)
    - ë” ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹…
    - rate limit ì²´í¬
    - fallback ëª¨ë¸ ì§€ì›
    - êµ¬ì¡°í™”ëœ í”¼ë“œë°±
    """
    if not req.content:
        return {"feedback": "âŒ ì´ë ¥ì„œ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}

    content = req.content.strip()

    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
    if len(content) > 5000:
        content = content[:5000]
        print(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ 5000ìë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ“ ì²¨ì‚­ ìš”ì²­ ë°›ìŒ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(content)}ì")

    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ IT ê¸°ì—…ì˜ ì „ë¬¸ ì±„ìš© ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ì´ë ¥ì„œì™€ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•˜ì—¬ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ê°œì„  ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.

**ì²¨ì‚­ ê¸°ì¤€:**
1. ëª…í™•ì„±: ëª¨í˜¸í•œ í‘œí˜„ì„ êµ¬ì²´ì ìœ¼ë¡œ ê°œì„ 
2. ì„íŒ©íŠ¸: ì„±ê³¼ì™€ ê¸°ì—¬ë„ë¥¼ ê°•ì¡°
3. êµ¬ì¡°: ë…¼ë¦¬ì  íë¦„ê³¼ ê°€ë…ì„±
4. ì „ë¬¸ì„±: IT ì§ë¬´ì— ì í•©í•œ í‘œí˜„
5. ë¬¸ë²•: ë§ì¶¤ë²•ê³¼ ë¬¸ì¥ êµ¬ì¡°

**ì‘ë‹µ í˜•ì‹:**
### ğŸ“Š ì „ì²´ í‰ê°€
- ì „ë°˜ì ì¸ ì¸ìƒê³¼ ê°•ì ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

### âœï¸ ì£¼ìš” ê°œì„ ì‚¬í•­
1. [í•­ëª©]: êµ¬ì²´ì ì¸ ë¬¸ì œì ê³¼ ê°œì„  ë°©í–¥
2. [í•­ëª©]: êµ¬ì²´ì ì¸ ë¬¸ì œì ê³¼ ê°œì„  ë°©í–¥
3. [í•­ëª©]: êµ¬ì²´ì ì¸ ë¬¸ì œì ê³¼ ê°œì„  ë°©í–¥

### ğŸ’¡ ì¶”ì²œ í‘œí˜„
- "ê¸°ì¡´ í‘œí˜„" â†’ "ê°œì„ ëœ í‘œí˜„"
- "ê¸°ì¡´ í‘œí˜„" â†’ "ê°œì„ ëœ í‘œí˜„"

### ğŸ¯ ë§ˆë¬´ë¦¬ ì¡°ì–¸
ì‹¤ìš©ì ì¸ ìµœì¢… ì¡°ì–¸ 1-2ë¬¸ì¥"""

    user_prompt = f"""ë‹¤ìŒ ë‚´ìš©ì„ ì²¨ì‚­í•´ì£¼ì„¸ìš”:

{content}

ìœ„ í˜•ì‹ì— ë§ì¶° êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì²¨ì‚­ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

    try:
        print("ğŸ¤– Gemini API í˜¸ì¶œ ì‹œì‘...")

        # Primary model: gemini-2.0-flash-exp (ë¬´ë£Œ)
        try:
            model = genai.GenerativeModel(
                model_name="models/gemini-2.0-flash-exp",
                system_instruction=system_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=1500,
                    temperature=0.4,
                    top_k=40,
                    top_p=0.95,
                )
            )

            response = model.generate_content(user_prompt)

            if response.candidates and response.candidates[0].content.parts:
                feedback = "\n".join([
                    part.text for part in response.candidates[0].content.parts
                    if hasattr(part, "text")
                ])

                if feedback and len(feedback) > 50:
                    print(f"âœ… Primary model ì‘ë‹µ ì„±ê³µ - ê¸¸ì´: {len(feedback)}ì")
                    return {"feedback": feedback}

            print("âš ï¸ Primary model ì‘ë‹µì´ ë¹„ì–´ìˆìŒ, fallback ì‹œë„...")

        except Exception as e:
            print(f"âš ï¸ Primary model ì‹¤íŒ¨: {str(e)[:100]}")

        # Fallback model: gemini-1.5-flash
        print("ğŸ”„ Fallback model ì‹œë„ ì¤‘...")

        model = genai.GenerativeModel(
            model_name="models/gemini-1.5-flash",
            system_instruction=system_prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=2048,
                temperature=0.4,
                stop_sequences=None,
            )
        )

        response = model.generate_content(user_prompt)

        if response.candidates and response.candidates[0].content.parts:
            feedback = "\n".join([
                part.text for part in response.candidates[0].content.parts
                if hasattr(part, "text")
            ])

            if feedback and len(feedback) > 50:
                print(f"âœ… Fallback model ì‘ë‹µ ì„±ê³µ - ê¸¸ì´: {len(feedback)}ì")

                # ì‘ë‹µì´ ì™„ì „í•œì§€ ì²´í¬
                if not feedback.rstrip().endswith(('.', '!', '?', 'ìš”', 'ë‹¤', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ì„¸ìš”')):
                    print(f"âš ï¸ Fallback ì‘ë‹µë„ ë¶ˆì™„ì „í•¨")
                    feedback += "\n\n### âš ï¸ ì‘ë‹µì´ ì¼ë¶€ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\nìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì‹œê³ , í•„ìš”ì‹œ ë‹¤ì‹œ ìš”ì²­í•´ì£¼ì„¸ìš”."

                return {"feedback": feedback}

        # ë‘ ëª¨ë¸ ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš°
        print("âŒ ëª¨ë“  ëª¨ë¸ì—ì„œ ì‘ë‹µ ì‹¤íŒ¨")
        return {
            "feedback": """### âš ï¸ AI ì²¨ì‚­ ì„œë¹„ìŠ¤ ì¼ì‹œ ì´ìš© ë¶ˆê°€

í˜„ì¬ AI ëª¨ë¸ì´ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:

1. **ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„**: API ìš”ì²­ ì œí•œì— ë„ë‹¬í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
2. **í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì´ê¸°**: ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ìš”ì•½í•´ì„œ ë‹¤ì‹œ ì‹œë„
3. **ê¸°ë³¸ ì²¨ì‚­ ê°€ì´ë“œ**:
   - êµ¬ì²´ì ì¸ ì„±ê³¼ ìˆ˜ì¹˜ í¬í•¨ (ì˜ˆ: "ë§¤ì¶œ 20% ì¦ê°€")
   - ê¸°ìˆ  ìŠ¤íƒ ëª…í™•íˆ ëª…ì‹œ
   - STAR ê¸°ë²• í™œìš© (ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼)
   - ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ì¥ ì‚¬ìš©

ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."""
        }

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ ì²¨ì‚­ API ì˜¤ë¥˜: {error_msg}")

        # ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
        if "quota" in error_msg.lower() or "limit" in error_msg.lower():
            return {
                "feedback": """### âš ï¸ API ì‚¬ìš©ëŸ‰ ì´ˆê³¼

í˜„ì¬ ì¼ì¼ API ì‚¬ìš©ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**ì„ì‹œ í•´ê²° ë°©ë²•:**
1. ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”
2. í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ ì„œ ì—¬ëŸ¬ ë²ˆ ì²¨ì‚­ ìš”ì²­
3. ì•„ë˜ ê¸°ë³¸ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”

**ê¸°ë³¸ ì²¨ì‚­ ê°€ì´ë“œ:**
- âœ… êµ¬ì²´ì ì¸ ìˆ«ìì™€ ì„±ê³¼ í¬í•¨
- âœ… ê¸°ìˆ  ìŠ¤íƒ ëª…í™•íˆ ê¸°ì¬
- âœ… ì£¼ìš” ê²½í—˜ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±
- âœ… ê°„ê²°í•˜ê³  ì„íŒ©íŠ¸ ìˆëŠ” í‘œí˜„ ì‚¬ìš©
- âŒ ì¶”ìƒì ì´ê³  ëª¨í˜¸í•œ í‘œí˜„ ì§€ì–‘"""
            }
        elif "timeout" in error_msg.lower():
            return {
                "feedback": """### â±ï¸ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼

API ì„œë²„ ì‘ë‹µì´ ì§€ì—°ë˜ê³  ìˆìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•:**
1. í…ìŠ¤íŠ¸ ì–‘ì„ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì‹œë„
2. ëª‡ ë¶„ í›„ì— ë‹¤ì‹œ ì‹œë„
3. ì„¹ì…˜ë³„ë¡œ ë‚˜ëˆ ì„œ ì²¨ì‚­ ìš”ì²­"""
            }
        else:
            return {
                "feedback": f"""### âŒ ì²¨ì‚­ ì˜¤ë¥˜ ë°œìƒ

ì˜¤ë¥˜ ë©”ì‹œì§€: {error_msg[:200]}

**ëŒ€ì²˜ ë°©ë²•:**
1. í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„
2. ë¸Œë¼ìš°ì € ìºì‹œ ì‚­ì œ
3. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜

**ê¸°ë³¸ ì²¨ì‚­ ì›ì¹™:**
- ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
- ì„±ê³¼ì™€ ê¸°ì—¬ë„ ê°•ì¡°
- ê¸°ìˆ  ìŠ¤íƒ ëª…í™•íˆ í‘œì‹œ
- STAR ê¸°ë²• í™œìš©"""
            }


# ì¶”ê°€: ì²¨ì‚­ ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/ai/review/health")
def review_health_check():
    """ì²¨ì‚­ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸"""
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        model = genai.GenerativeModel("models/gemini-2.0-flash-exp")
        response = model.generate_content(
            "ê°„ë‹¨íˆ 'ì •ìƒ'ì´ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=10,
                temperature=0.1,
            )
        )

        if response.candidates:
            return {
                "status": "healthy",
                "message": "AI ì²¨ì‚­ ì„œë¹„ìŠ¤ ì •ìƒ ì‘ë™ ì¤‘",
                "model": "gemini-2.0-flash-exp"
            }
    except Exception as e:
        return {
            "status": "unhealthy",
            "message": f"ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€: {str(e)[:100]}",
            "model": "none"
        }


@app.get("/ai/review/health")
def review_health_check():
    """ì²¨ì‚­ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸"""
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        model = genai.GenerativeModel("models/gemini-2.0-flash-exp")
        response = model.generate_content(
            "ê°„ë‹¨íˆ 'ì •ìƒ'ì´ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=10,
                temperature=0.1,
            )
        )

        if response.candidates:
            return {
                "status": "healthy",
                "message": "AI ì²¨ì‚­ ì„œë¹„ìŠ¤ ì •ìƒ ì‘ë™ ì¤‘",
                "model": "gemini-2.0-flash-exp"
            }
    except Exception as e:
        return {
            "status": "unhealthy",
            "message": f"ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€: {str(e)[:100]}",
            "model": "none"
        }


# ------------------------------------------------------
# Embedding (ì˜µì…˜ 2)
# ------------------------------------------------------

class SummRequest(BaseModel):
    text: str


@app.post("/ai/summarize")
def summarize(req: SummRequest):
    text = req.text.strip()

    if not text:
        return {"summary": "ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}

    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (Gemini í† í° ì œí•œ ê³ ë ¤)
    if len(text) > 8000:
        text = text[:4000] + "\n...(ì¤‘ëµ)...\n" + text[-2000:]

    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ IT ê¸°ì—… ì±„ìš©ê³µê³  ì „ë¬¸ ìš”ì•½ê¸°ë‹¤.\n"
        "ê·œì¹™:\n"
        "1. ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ 5ê°œë¥¼ ì‘ì„±í•œë‹¤.\n"
        "2. ê° ë¬¸ì¥ì€ ë§ˆì¹¨í‘œ(.)ë¡œ ëë‚˜ì•¼ í•œë‹¤.\n"
        "3. ë¶ˆì™„ì „í•œ ë¬¸ì¥ì€ ì ˆëŒ€ ê¸ˆì§€ë‹¤.\n"
        "4. ë‹¤ìŒ ìˆœì„œë¡œ ì‘ì„±:\n"
        "   - 1ë¬¸ì¥: íšŒì‚¬ ì†Œê°œ ë˜ëŠ” ì„œë¹„ìŠ¤ ì„¤ëª…\n"
        "   - 2ë¬¸ì¥: ì£¼ìš” ì—…ë¬´ ë‚´ìš©\n"
        "   - 1ë¬¸ì¥: í•„ìˆ˜ ìê²©ìš”ê±´ (ê¸°ìˆ ìŠ¤íƒ í¬í•¨)\n"
        "   - 1ë¬¸ì¥: ìš°ëŒ€ì‚¬í•­ ë˜ëŠ” ë³µì§€\n"
        "5. ê° ë¬¸ì¥ì€ ìµœì†Œ 10ë‹¨ì–´ ì´ìƒì´ì–´ì•¼ í•œë‹¤.\n"
        "6. 'ë¦¬ë·°', 'ë¶„ì„', 'ì¡°ì–¸' ê°™ì€ ë©”íƒ€ ì–¸ê¸‰ ê¸ˆì§€.\n"
        "7. ì •ì§í•˜ê³  ê°„ê²°í•œ ë¬¸ì–´ì²´ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.\n"
    )

    user_prompt = (
        f"ë‹¤ìŒ ì±„ìš©ê³µê³ ë¥¼ ì •í™•íˆ 5ê°œì˜ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n"
        f"ê° ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"ì±„ìš©ê³µê³  ë‚´ìš©:\n{text}\n\n"
        f"5ê°œ ë¬¸ì¥ ìš”ì•½ (ê° ë¬¸ì¥ì€ ë§ˆì¹¨í‘œë¡œ ëë‚¨):"
    )

    # 1ì°¨ ì‹œë„
    out = generate_text(
        sys_prompt,
        user_prompt,
        max_tokens=800,  # í† í° ìˆ˜ ì¦ê°€
        temperature=0.3,
    )

    # ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° 2ì°¨ ì‹œë„
    if not out or len(out.strip()) < 100:
        print(f"âš ï¸ 1ì°¨ ìš”ì•½ ì‹¤íŒ¨, 2ì°¨ ì‹œë„ ì¤‘... (ê¸¸ì´: {len(out) if out else 0})")

        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
        simple_prompt = (
            f"ë‹¤ìŒ ì±„ìš©ê³µê³ ë¥¼ 5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n\n{text[:3000]}\n\n"
            f"ìš”ì•½ (5ë¬¸ì¥):"
        )

        out = generate_text(
            "ë„ˆëŠ” ì±„ìš©ê³µê³  ìš”ì•½ ì „ë¬¸ê°€ë‹¤. í•­ìƒ 5ê°œì˜ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œë‹¤.",
            simple_prompt,
            max_tokens=800,
            temperature=0.4,
        )

    # ì—¬ì „íˆ ì‹¤íŒ¨í•œ ê²½ìš°
    if not out or len(out.strip()) < 50:
        return {"summary": "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}

    # ë¬¸ì¥ ê²€ì¦
    lines = [l.strip() for l in out.split("\n") if l.strip() and not l.strip().startswith("(")]
    sentences = []

    for line in lines:
        # ë§ˆì¹¨í‘œë¡œ ë¬¸ì¥ ë¶„ë¦¬
        parts = [s.strip() + "." for s in line.split(".") if s.strip() and len(s.strip()) > 5]
        sentences.extend(parts)

    # ìµœì†Œ 3ë¬¸ì¥ ì´ìƒ í™•ë³´
    if len(sentences) < 3:
        print(f"âš ï¸ ë¬¸ì¥ ìˆ˜ ë¶€ì¡±: {len(sentences)}ê°œ")
        # ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìµœì†Œí•œì˜ ë‚´ìš©ì´ë¼ë„ ì €ì¥)
        return {"summary": out.strip()}

    # ìƒìœ„ 5ë¬¸ì¥ë§Œ ì„ íƒ
    final_summary = " ".join(sentences[:5])

    return {"summary": final_summary}


@app.post("/ai/embed")
def embed(req: EmbedRequest):
    text = req.text.strip()
    if not text:
        return {"vector": []}

    try:
        emb = genai.embed_content(
            model="models/text-embedding-004",
            content=text
        )
        return {"vector": emb["embedding"]}
    except Exception as e:
        print("[EMBED ERROR]", e)
        return {"vector": []}


# ------------------------------------------------------
# match-one (ì˜µì…˜ 4)
# ------------------------------------------------------

@app.post("/ai/match-one")
def match_one(req: MatchOneRequest):
    """
    ë¬´ë£Œ í‹°ì–´ ìµœì í™” ë²„ì „
    - gemini-2.0-flash-exp ì‚¬ìš© (ì™„ì „ ë¬´ë£Œ)
    - rate limit: ë¶„ë‹¹ 15íšŒ, ì¼ì¼ 1,500íšŒ
    """
    resume = req.resume or ""
    job = req.job or ""

    if not resume.strip() or not job.strip():
        return {"score": 0, "reason": "ì…ë ¥ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}

    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œìœ¼ë¡œ í† í° ì ˆì•½
    resume = resume[:1000]
    job = job[:1000]

    try:
        # âœ… ì™„ì „ ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš©
        model = genai.GenerativeModel(
            model_name="models/gemini-2.0-flash-exp",  # ë¬´ë£Œ!
            system_instruction="""You are a Korean job matching AI.
Evaluate resume-job match and respond with score (0-100) and Korean reason.

Scoring guide:
90-100: Perfect match
70-89: Good match
50-69: Fair match
30-49: Poor match
0-29: No match""",
            generation_config=genai.types.GenerationConfig(
                temperature=0.1,
                max_output_tokens=200,  # í† í° ì ˆì•½
                response_mime_type="application/json",
                response_schema={
                    "type": "object",
                    "properties": {
                        "score": {"type": "integer"},
                        "reason": {"type": "string"}
                    },
                    "required": ["score", "reason"]
                }
            )
        )

        prompt = f"""Resume: {resume}

Job: {job}

Evaluate match."""

        response = model.generate_content(prompt)
        result = json.loads(response.text)

        score = int(min(max(result["score"], 0), 100))
        reason = result["reason"]

        return {"score": score, "reason": reason}

    except Exception as e:
        print(f"âŒ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
        # í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±
        return keyword_based_match(resume, job)


def keyword_based_match(resume: str, job: str):
    """API ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±"""
    resume_lower = resume.lower()
    job_lower = job.lower()

    tech_keywords = {
        "backend": ["python", "java", "spring", "django", "node", "go", "kotlin"],
        "frontend": ["react", "vue", "angular", "javascript", "typescript", "css"],
        "database": ["mysql", "postgresql", "mongodb", "redis", "oracle"],
        "devops": ["aws", "docker", "kubernetes", "jenkins", "git", "linux"],
        "mobile": ["android", "ios", "swift", "flutter", "react native"]
    }

    total_matches = 0
    for keywords in tech_keywords.values():
        matches = sum(1 for kw in keywords if kw in resume_lower and kw in job_lower)
        total_matches += matches

    final_score = min(total_matches * 7, 70)
    reason = f"í‚¤ì›Œë“œ ë§¤ì¹­ ({total_matches}ê°œ ì¼ì¹˜)"

    return {"score": final_score, "reason": reason}

# ------------------------------------------------------
# ìƒíƒœ & ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
# ------------------------------------------------------

@app.get("/health")
def health():
    return {"ok": True}

@app.get("/ai/models")
def list_models():
    try:
        return [{"name": m.name} for m in genai.list_models()]
    except Exception as e:
        return {"error": str(e)}


# Rate limiter ë°ì½”ë ˆì´í„°
def rate_limit(calls_per_minute=15):
    """
    Gemini APIì˜ ê¸°ë³¸ rate limitì€ ë¶„ë‹¹ 15íšŒ
    """
    min_interval = 60.0 / calls_per_minute
    last_called = [0.0]

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed

            if left_to_wait > 0:
                time.sleep(left_to_wait)

            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret

        return wrapper

    return decorator


# generate_text í•¨ìˆ˜ì— ì ìš©
@rate_limit(calls_per_minute=15)  # ë¶„ë‹¹ 15íšŒë¡œ ì œí•œ
def generate_text_safe(system, prompt, max_tokens=512, temperature=0.3):
    """Rate limitì´ ì ìš©ëœ ì•ˆì „í•œ ë²„ì „"""
    return generate_text(system, prompt, max_tokens, temperature)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)
