# main.py â€” FULL Combined Version
# ë‰´ìŠ¤ / moderation / review / match-one / embed / models / health ì „ë¶€ í¬í•¨
# + ì„ë² ë”© ê¸°ë°˜ Spring ë§¤ì¹­ ì§€ì›
# + 10ë§Œ job_posts í™•ì¥ì„ ìœ„í•œ ì†ë„ ìµœì í™” (ì„ë² ë”© ìºì‹œ-friendly)

import os, sys, re, json, hashlib
import requests
import google.generativeai as genai
from datetime import datetime, timedelta
from typing import List, Optional
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import dotenv_values, load_dotenv

sys.stdout.reconfigure(encoding='utf-8')

sys.stdout.reconfigure(encoding='utf-8')

# ===== ENV ì ìš© =====
env = dotenv_values(".env")
for k, v in env.items():
    if v:
        os.environ[k] = v
load_dotenv()
import time
from functools import wraps
from typing_extensions import TypedDict
from collections import deque
from fastapi.responses import StreamingResponse
import asyncio

# ===== Gemini ì„¤ì • =====
API_KEY = os.getenv("GEMINI_API_KEY", "")
genai.configure(api_key=API_KEY)

print("âœ… FastAPI Loaded â€” ALL FEATURES READY")


# ------------------------------------------------------
# ê³µí†µ ìœ í‹¸
# ------------------------------------------------------

def safe_json(s: str):
    """JSON íŒŒì‹± ì•ˆì „ ë²„ì „"""
    if not s:
        return {}
    try:
        return json.loads(s)
    except:
        m = re.search(r"\{.*\}", s, re.S)
        if not m:
            return {}
        try:
            return json.loads(m.group(0))
        except:
            return {}


def safe_json_v2(s: str):
    """
    Gemini API ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ (ê°œì„  ë²„ì „)
    """
    if not s:
        return {}

    # 1. Markdown ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ```)
    s = re.sub(r'```json\s*', '', s)
    s = re.sub(r'```\s*$', '', s)
    s = re.sub(r'^```\s*', '', s)

    # 2. ì•ë’¤ ê³µë°± ì œê±°
    s = s.strip()

    # 3. ì§ì ‘ íŒŒì‹± ì‹œë„
    try:
        return json.loads(s)
    except:
        pass

    # 4. JSON ê°ì²´ë§Œ ì¶”ì¶œ ì‹œë„
    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', s, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass

    # 5. ìµœí›„ì˜ ìˆ˜ë‹¨: scoreì™€ reasonë§Œ ì¶”ì¶œ
    try:
        score_match = re.search(r'"score"\s*:\s*(\d+)', s)
        reason_match = re.search(r'"reason"\s*:\s*"([^"]*)"', s)

        if score_match:
            score = int(score_match.group(1))
            reason = reason_match.group(1) if reason_match else "ë¶„ì„ ì™„ë£Œ"
            return {"score": score, "reason": reason}
    except:
        pass

    return {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}


# â­ ì´ í•¨ìˆ˜ê°€ ë¨¼ì € ì •ì˜ë˜ì–´ì•¼ í•¨!
def call_llm(model, system, prompt, max_tokens=512, temperature=0.3):
    """LLM í˜¸ì¶œ (ëª¨ë“  ê¸°ëŠ¥ ê³µí†µ ì‚¬ìš©)"""
    try:
        m = genai.GenerativeModel(
            model_name=model,
            system_instruction=system
        )
        r = m.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
                top_k=1,
                top_p=0.95,
                candidate_count=1,
            )
        )

        cand = r.candidates[0] if r.candidates else None
        if cand and cand.content.parts:
            return "\n".join([p.text for p in cand.content.parts if getattr(p, "text", None)])
    except Exception as e:
        print(f"[LLM ERROR] {e}")
    return None


def call_llm_with_json(model, system, prompt, max_tokens=512, temperature=0.3):
    """JSON ì‘ë‹µì„ ê°•ì œí•˜ëŠ” LLM í˜¸ì¶œ"""
    try:
        m = genai.GenerativeModel(
            model_name=model,
            system_instruction=system,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
                top_k=1,
                top_p=0.95,
                candidate_count=1,
                response_mime_type="application/json",  # â­ JSON ê°•ì œ
            )
        )

        r = m.generate_content(prompt)

        cand = r.candidates[0] if r.candidates else None
        if cand and cand.content.parts:
            text = "\n".join([p.text for p in cand.content.parts if getattr(p, "text", None)])
            return text
    except Exception as e:
        print(f"[LLM JSON ERROR] {e}")

    return None


def generate_text(system, prompt, max_tokens=512, temperature=0.3):
    """2ë‹¨ê³„ ëª¨ë¸ í´ë°± í¬í•¨"""
    primary = "gemini-2.0-flash"
    fallback = "gemini-2.0-flash"

    out = call_llm(primary, system, prompt, max_tokens, temperature)
    if out:
        return out

    out = call_llm(fallback, system, prompt, max_tokens, temperature)
    return out or "âš ï¸ ëª¨ë¸ ë¬´ì‘ë‹µ"


def generate_json(system, user_prompt, schema_hint, max_tokens=512, temperature=0.2):
    """JSONë§Œ ê°•ì œë¡œ ë½‘ì•„ì˜¤ëŠ” í—¬í¼"""
    prompt = (
        f"{user_prompt}\n\në°˜ë“œì‹œ JSONë§Œ ì¶œë ¥.\nìŠ¤í‚¤ë§ˆ: {schema_hint}"
    )
    out = generate_text(system, prompt, max_tokens, temperature)
    data = safe_json(out)
    if data:
        return data
    return safe_json(out)

# save_chat_to_spring í•¨ìˆ˜ êµ¬í˜„
SPRING_URL = "http://localhost:8080/api/chatbot/save"

def save_chat_to_spring(user_id, session_id, user_message, bot_answer):
    payload = {
        "userId": user_id,
        "sessionId": str(session_id),
        "userMessage": user_message,
        "botAnswer": bot_answer
    }

    try:
        r = requests.post(SPRING_URL, json=payload, timeout=5)
        print("ğŸ“Œ Chat saved:", r.status_code)
    except Exception as e:
        print("âŒ Chat save failed:", e)

# ------------------------------------------------------
# FastAPI ì„¤ì •
# ------------------------------------------------------

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# ------------------------------------------------------
# Schemas
# ------------------------------------------------------

class NewsItem(BaseModel):
    title: str
    link: str
    description: Optional[str] = None
    pubDate: Optional[str] = None
    press: Optional[str] = None

class DigestRequest(BaseModel):
    query: str = "ì±„ìš© OR ê³µì±„"
    days: int = 3
    limit: int = 20
    style: str = "bullet"

class ReviewRequest(BaseModel):
    content: str

class MatchOneRequest(BaseModel):
    resume: Optional[str] = ""
    job: Optional[str] = ""

class EmbedRequest(BaseModel):
    text: str


class RateLimiter:
    """ë¬´ë£Œ í‹°ì–´ Rate Limit ê´€ë¦¬"""

    def __init__(self, rpm=15, rpd=1500):
        self.rpm = rpm  # ë¶„ë‹¹ ìš”ì²­
        self.rpd = rpd  # ì¼ì¼ ìš”ì²­
        self.minute_queue = deque()
        self.daily_count = 0
        self.daily_reset = datetime.now() + timedelta(days=1)

    def check_and_wait(self):
        """Rate limit ì²´í¬ ë° ëŒ€ê¸°"""
        now = datetime.now()

        # ì¼ì¼ ì œí•œ ë¦¬ì…‹
        if now >= self.daily_reset:
            self.daily_count = 0
            self.daily_reset = now + timedelta(days=1)

        # ì¼ì¼ ì œí•œ ì²´í¬
        if self.daily_count >= self.rpd:
            raise Exception(f"ì¼ì¼ ìš”ì²­ ì œí•œ ì´ˆê³¼ ({self.rpd}íšŒ)")

        # 1ë¶„ ì´ì „ ìš”ì²­ ì œê±°
        one_minute_ago = now - timedelta(minutes=1)
        while self.minute_queue and self.minute_queue[0] < one_minute_ago:
            self.minute_queue.popleft()

        # ë¶„ë‹¹ ì œí•œ ì²´í¬ ë° ëŒ€ê¸°
        if len(self.minute_queue) >= self.rpm:
            # ê°€ì¥ ì˜¤ë˜ëœ ìš”ì²­ì´ 1ë¶„ ê²½ê³¼í•  ë•Œê¹Œì§€ ëŒ€ê¸°
            wait_until = self.minute_queue[0] + timedelta(minutes=1)
            wait_seconds = (wait_until - now).total_seconds()
            if wait_seconds > 0:
                print(f"â³ Rate limit ëŒ€ê¸°: {wait_seconds:.1f}ì´ˆ")
                time.sleep(wait_seconds + 0.1)

        # ìš”ì²­ ê¸°ë¡
        self.minute_queue.append(now)
        self.daily_count += 1

        return True


# ì „ì—­ Rate Limiter
gemini_limiter = RateLimiter(rpm=15, rpd=1500)

# ------------------------------------------------------
# NAVER ë‰´ìŠ¤ íŒŒíŠ¸
# ------------------------------------------------------

def naver_news_search(query, start=1, display=20, sort="date"):
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {"query": query, "display": display, "start": start, "sort": sort}
    r = requests.get("https://openapi.naver.com/v1/search/news.json",
                     headers=headers, params=params, timeout=10)
    r.raise_for_status()
    return r.json()



@app.post("/news/fetch", response_model=List[NewsItem])
def fetch_news(req: DigestRequest):
    items = []
    remaining = req.limit
    start = 1

    while remaining > 0 and start <= 1000:
        size = min(20, remaining)
        data = naver_news_search(req.query, start, size)
        raw = data.get("items", [])
        if not raw:
            break

        for it in raw:
            items.append(
                NewsItem(
                    title=it["title"].replace("<b>", "").replace("</b>", ""),
                    link=it["link"],
                    description=it.get("description", "").replace("<b>", "").replace("</b>", ""),
                    pubDate=it.get("pubDate"),
                    press=it.get("originallink"),
                )
            )
        remaining -= len(raw)
        start += size

    # ë‚ ì§œ í•„í„°
    if req.days > 0:
        cutoff = datetime.utcnow() - timedelta(days=req.days + 0.5)

        def in_range(n):
            try:
                dt = datetime.strptime(n.pubDate, "%a, %d %b %Y %H:%M:%S %z")
                return dt.timestamp() >= cutoff.timestamp()
            except:
                return False

        items = [n for n in items if in_range(n)]

    return items[:req.limit]


@app.post("/news/digest")
def news_digest(req: DigestRequest):
    queries = [
        "ì±„ìš© OR ê³µì±„ OR ë…¸ë™ì‹œì¥",
        "IT OR ê¸°ìˆ  OR ê°œë°œì OR AI",
        "ê²½ì œ OR ì‚°ì—… OR ì‹œì¥"
    ]

    items = []
    for q in queries:
        items = fetch_news(DigestRequest(query=q, days=req.days, limit=req.limit))
        if items:
            break

    if not items:
        return {"title": "ë‰´ìŠ¤ ì—†ìŒ", "content": "", "tags": [], "sources": []}

    titles = "\n".join(f"- {n.title}" for n in items)

    out = generate_text(
        "ë„ˆëŠ” ë‰´ìŠ¤ ì „ë¬¸ í¸ì§‘ìë‹¤. ìš”ì•½ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , 'ì•Œê² ìŠµë‹ˆë‹¤', 'ë‹¤ìŒê³¼ ê°™ì´' ê°™ì€ ì¸ì‚¬ë§ì´ë‚˜ ì„œë¡  ì—†ì´ ë°”ë¡œ ë³¸ë¬¸ì„ ì‹œì‘í•´ë¼.",
        f"""ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ë“¤ì„ {req.style} ìŠ¤íƒ€ì¼ë¡œ ìš”ì•½í•´ë¼. ì¸ì‚¬ë§ ì—†ì´ ë°”ë¡œ ìš”ì•½ ë‚´ìš©ë§Œ ì‘ì„±:
{titles}
""",
        max_tokens=500,
        temperature=0.25
    )

    # ğŸ”¥ ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§ ì œê±°
    content = out.strip()

    # "ì•Œê² ìŠµë‹ˆë‹¤", "ë‹¤ìŒê³¼ ê°™ì´" ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì²« ì¤„ ì œê±°
    lines = content.split('\n')
    if lines and any(phrase in lines[0] for phrase in ['ì•Œê² ìŠµë‹ˆë‹¤', 'ë‹¤ìŒê³¼ ê°™ì´', 'ìš”ì•½í–ˆìŠµë‹ˆë‹¤', 'ì •ë¦¬í–ˆìŠµë‹ˆë‹¤', 'ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤']):
        content = '\n'.join(lines[1:]).strip()

    uid = datetime.now().strftime("%Y%m%d%H%M%S")
    return {
        "title": f"AI ë‰´ìŠ¤ ìš”ì•½ ({uid})",
        "content": content,
        "tags": ["ë‰´ìŠ¤", "ìš”ì•½"],
        "sources": items,
    }


# ------------------------------------------------------
# Moderation
# ------------------------------------------------------

KOREAN_BAD_RE = re.compile(r"(ì”¨ë°œ|ì‹œë°œ|ã……\s*ã…‚|ê°œ\s*ìƒˆë¼|ë³‘ì‹ |ì¢†|ì§€ë„|ì”¹)", re.I)
SPAM_RE = [r"(http|https)://", r"ì˜¤í”ˆì±„íŒ…", r"ìˆ˜ìµ\s?ë³´ì¥", r"ìƒë‹´\s?ë¬¸ì˜"]

# ê²€ì—´ ê²°ê³¼ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜, ìµœëŒ€ 1000ê°œ)
moderation_cache = {}
MAX_CACHE_SIZE = 1000

@app.post("/ai/moderate")
def moderate(req: dict):
    text = (req or {}).get("content", "") or ""

    # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì¦‰ì‹œ ìŠ¹ì¸
    if not text or len(text.strip()) < 2:
        return {"approve": True, "reason": "ë¹ˆ ë‚´ìš©", "categories": {}}

    # ìºì‹œ í™•ì¸ (ê°™ì€ ë‚´ìš©ì€ ì¬ê²€ì‚¬ ì•ˆ í•¨)
    text_hash = hashlib.md5(text.encode()).hexdigest()
    if text_hash in moderation_cache:
        return moderation_cache[text_hash]

    # 1ì°¨: ëª…ë°±í•œ ìš•ì„¤/ìŠ¤íŒ¸ì€ ì¦‰ì‹œ ì°¨ë‹¨ (AI í˜¸ì¶œ ì—†ìŒ)
    if KOREAN_BAD_RE.search(text) or any(re.search(p, text, re.I) for p in SPAM_RE):
        result = {"approve": False, "reason": "ë£° ê¸°ë°˜ ì°¨ë‹¨", "categories": {"rule": 1.0}}
        moderation_cache[text_hash] = result
        return result

    # 2ì°¨: ëª¨ë“  í…ìŠ¤íŠ¸ AI ê²€ì‚¬
    data = generate_json(
        "ë„ˆëŠ” ì½˜í…ì¸  ì•ˆì „ ì‹¬ì‚¬ê´€ì´ë‹¤.",
        f"ì•„ë˜ ê¸€ ìœ„í—˜ë„ ë¶„ì„:\n{text[:500]}",  # 500ìë¡œ ì œí•œ
        schema_hint='{"approve": bool, "categories": {}, "reason": ""}'
    )

    cats = data.get("categories", {})
    risk_values = [float(v) for v in cats.values()] if cats else []
    max_risk = max(risk_values) if risk_values else 0

    approve = data.get("approve", True)
    if max_risk >= 0.5:
        approve = False

    result = {"approve": approve, "reason": data.get("reason", ""), "categories": cats}

    # ìºì‹œ ì €ì¥ (í¬ê¸° ì œí•œ)
    if len(moderation_cache) < MAX_CACHE_SIZE:
        moderation_cache[text_hash] = result

    return result


# ------------------------------------------------------
# Review
# ------------------------------------------------------

@app.post("/ai/review")
def review_resume(req: ReviewRequest):
    """
    ì´ë ¥ì„œ/ìì†Œì„œ ì²¨ì‚­ ê¸°ëŠ¥ (ê°œì„  ë²„ì „)
    - ë” ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹…
    - rate limit ì²´í¬
    - fallback ëª¨ë¸ ì§€ì›
    - êµ¬ì¡°í™”ëœ í”¼ë“œë°±
    """
    if not req.content:
        return {"feedback": "âŒ ì´ë ¥ì„œ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}

    content = req.content.strip()

    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
    if len(content) > 5000:
        content = content[:5000]
        print(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ 5000ìë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ“ ì²¨ì‚­ ìš”ì²­ ë°›ìŒ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(content)}ì")

    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ IT ê¸°ì—…ì˜ ì „ë¬¸ ì±„ìš© ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ì´ë ¥ì„œì™€ ìê¸°ì†Œê°œì„œë¥¼ ì²¨ì‚­í•˜ì—¬ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ê°œì„  ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.

**ì²¨ì‚­ ê¸°ì¤€:**
1. ëª…í™•ì„±: ëª¨í˜¸í•œ í‘œí˜„ì„ êµ¬ì²´ì ìœ¼ë¡œ ê°œì„ 
2. ì„íŒ©íŠ¸: ì„±ê³¼ì™€ ê¸°ì—¬ë„ë¥¼ ê°•ì¡°
3. êµ¬ì¡°: ë…¼ë¦¬ì  íë¦„ê³¼ ê°€ë…ì„±
4. ì „ë¬¸ì„±: IT ì§ë¬´ì— ì í•©í•œ í‘œí˜„
5. ë¬¸ë²•: ë§ì¶¤ë²•ê³¼ ë¬¸ì¥ êµ¬ì¡°

**ì‘ë‹µ í˜•ì‹:**
### ğŸ“Š ì „ì²´ í‰ê°€
- ì „ë°˜ì ì¸ ì¸ìƒê³¼ ê°•ì ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

### âœï¸ ì£¼ìš” ê°œì„ ì‚¬í•­
1. [í•­ëª©]: êµ¬ì²´ì ì¸ ë¬¸ì œì ê³¼ ê°œì„  ë°©í–¥
2. [í•­ëª©]: êµ¬ì²´ì ì¸ ë¬¸ì œì ê³¼ ê°œì„  ë°©í–¥
3. [í•­ëª©]: êµ¬ì²´ì ì¸ ë¬¸ì œì ê³¼ ê°œì„  ë°©í–¥

### ğŸ’¡ ì¶”ì²œ í‘œí˜„
- "ê¸°ì¡´ í‘œí˜„" â†’ "ê°œì„ ëœ í‘œí˜„"
- "ê¸°ì¡´ í‘œí˜„" â†’ "ê°œì„ ëœ í‘œí˜„"

### ğŸ¯ ë§ˆë¬´ë¦¬ ì¡°ì–¸
ì‹¤ìš©ì ì¸ ìµœì¢… ì¡°ì–¸ 1-2ë¬¸ì¥"""

    user_prompt = f"""ë‹¤ìŒ ë‚´ìš©ì„ ì²¨ì‚­í•´ì£¼ì„¸ìš”:

{content}

ìœ„ í˜•ì‹ì— ë§ì¶° êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì²¨ì‚­ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

    try:
        print("ğŸ¤– Gemini API í˜¸ì¶œ ì‹œì‘...")

        # Primary model: gemini-2.0-flash
        try:
            model = genai.GenerativeModel(
                model_name="gemini-2.0-flash",
                system_instruction=system_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=1500,
                    temperature=0.4,
                    top_k=40,
                    top_p=0.95,
                )
            )

            response = model.generate_content(user_prompt)

            if response.candidates and response.candidates[0].content.parts:
                feedback = "\n".join([
                    part.text for part in response.candidates[0].content.parts
                    if hasattr(part, "text")
                ])

                if feedback and len(feedback) > 50:
                    print(f"âœ… Primary model ì‘ë‹µ ì„±ê³µ - ê¸¸ì´: {len(feedback)}ì")
                    return {"feedback": feedback}

            print("âš ï¸ Primary model ì‘ë‹µì´ ë¹„ì–´ìˆìŒ, fallback ì‹œë„...")

        except Exception as e:
            print(f"âš ï¸ Primary model ì‹¤íŒ¨: {str(e)[:100]}")

        # Fallback model: gemini-2.0-flash
        print("ğŸ”„ Fallback model ì‹œë„ ì¤‘...")

        model = genai.GenerativeModel(
            model_name="gemini-2.0-flash",
            system_instruction=system_prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=2048,
                temperature=0.4,
                stop_sequences=None,
            )
        )

        response = model.generate_content(user_prompt)

        if response.candidates and response.candidates[0].content.parts:
            feedback = "\n".join([
                part.text for part in response.candidates[0].content.parts
                if hasattr(part, "text")
            ])

            if feedback and len(feedback) > 50:
                print(f"âœ… Fallback model ì‘ë‹µ ì„±ê³µ - ê¸¸ì´: {len(feedback)}ì")

                # ì‘ë‹µì´ ì™„ì „í•œì§€ ì²´í¬
                if not feedback.rstrip().endswith(('.', '!', '?', 'ìš”', 'ë‹¤', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ì„¸ìš”')):
                    print(f"âš ï¸ Fallback ì‘ë‹µë„ ë¶ˆì™„ì „í•¨")
                    feedback += "\n\n### âš ï¸ ì‘ë‹µì´ ì¼ë¶€ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\nìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì‹œê³ , í•„ìš”ì‹œ ë‹¤ì‹œ ìš”ì²­í•´ì£¼ì„¸ìš”."

                return {"feedback": feedback}

        # ë‘ ëª¨ë¸ ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš°
        print("âŒ ëª¨ë“  ëª¨ë¸ì—ì„œ ì‘ë‹µ ì‹¤íŒ¨")
        return {
            "feedback": """### âš ï¸ AI ì²¨ì‚­ ì„œë¹„ìŠ¤ ì¼ì‹œ ì´ìš© ë¶ˆê°€

í˜„ì¬ AI ëª¨ë¸ì´ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:

1. **ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„**: API ìš”ì²­ ì œí•œì— ë„ë‹¬í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
2. **í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì´ê¸°**: ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ìš”ì•½í•´ì„œ ë‹¤ì‹œ ì‹œë„
3. **ê¸°ë³¸ ì²¨ì‚­ ê°€ì´ë“œ**:
   - êµ¬ì²´ì ì¸ ì„±ê³¼ ìˆ˜ì¹˜ í¬í•¨ (ì˜ˆ: "ë§¤ì¶œ 20% ì¦ê°€")
   - ê¸°ìˆ  ìŠ¤íƒ ëª…í™•íˆ ëª…ì‹œ
   - STAR ê¸°ë²• í™œìš© (ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼)
   - ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ì¥ ì‚¬ìš©

ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."""
        }

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ ì²¨ì‚­ API ì˜¤ë¥˜: {error_msg}")

        # ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
        if "quota" in error_msg.lower() or "limit" in error_msg.lower():
            return {
                "feedback": """### âš ï¸ API ì‚¬ìš©ëŸ‰ ì´ˆê³¼

í˜„ì¬ ì¼ì¼ API ì‚¬ìš©ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**ì„ì‹œ í•´ê²° ë°©ë²•:**
1. ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”
2. í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ ì„œ ì—¬ëŸ¬ ë²ˆ ì²¨ì‚­ ìš”ì²­
3. ì•„ë˜ ê¸°ë³¸ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”

**ê¸°ë³¸ ì²¨ì‚­ ê°€ì´ë“œ:**
- âœ… êµ¬ì²´ì ì¸ ìˆ«ìì™€ ì„±ê³¼ í¬í•¨
- âœ… ê¸°ìˆ  ìŠ¤íƒ ëª…í™•íˆ ê¸°ì¬
- âœ… ì£¼ìš” ê²½í—˜ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±
- âœ… ê°„ê²°í•˜ê³  ì„íŒ©íŠ¸ ìˆëŠ” í‘œí˜„ ì‚¬ìš©
- âŒ ì¶”ìƒì ì´ê³  ëª¨í˜¸í•œ í‘œí˜„ ì§€ì–‘"""
            }
        elif "timeout" in error_msg.lower():
            return {
                "feedback": """### â±ï¸ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼

API ì„œë²„ ì‘ë‹µì´ ì§€ì—°ë˜ê³  ìˆìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•:**
1. í…ìŠ¤íŠ¸ ì–‘ì„ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì‹œë„
2. ëª‡ ë¶„ í›„ì— ë‹¤ì‹œ ì‹œë„
3. ì„¹ì…˜ë³„ë¡œ ë‚˜ëˆ ì„œ ì²¨ì‚­ ìš”ì²­"""
            }
        else:
            return {
                "feedback": f"""### âŒ ì²¨ì‚­ ì˜¤ë¥˜ ë°œìƒ

ì˜¤ë¥˜ ë©”ì‹œì§€: {error_msg[:200]}

**ëŒ€ì²˜ ë°©ë²•:**
1. í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„
2. ë¸Œë¼ìš°ì € ìºì‹œ ì‚­ì œ
3. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜

**ê¸°ë³¸ ì²¨ì‚­ ì›ì¹™:**
- ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
- ì„±ê³¼ì™€ ê¸°ì—¬ë„ ê°•ì¡°
- ê¸°ìˆ  ìŠ¤íƒ ëª…í™•íˆ í‘œì‹œ
- STAR ê¸°ë²• í™œìš©"""
            }


# ì¶”ê°€: ì²¨ì‚­ ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/ai/review/health")
def review_health_check():
    """ì²¨ì‚­ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸"""
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(
            "ê°„ë‹¨íˆ 'ì •ìƒ'ì´ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=10,
                temperature=0.1,
            )
        )

        if response.candidates:
            return {
                "status": "healthy",
                "message": "AI ì²¨ì‚­ ì„œë¹„ìŠ¤ ì •ìƒ ì‘ë™ ì¤‘",
                "model": "gemini-2.0-flash"
            }
    except Exception as e:
        return {
            "status": "unhealthy",
            "message": f"ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€: {str(e)[:100]}",
            "model": "none"
        }


@app.get("/ai/review/health")
def review_health_check():
    """ì²¨ì‚­ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸"""
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(
            "ê°„ë‹¨íˆ 'ì •ìƒ'ì´ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=10,
                temperature=0.1,
            )
        )

        if response.candidates:
            return {
                "status": "healthy",
                "message": "AI ì²¨ì‚­ ì„œë¹„ìŠ¤ ì •ìƒ ì‘ë™ ì¤‘",
                "model": "gemini-2.0-flash"
            }
    except Exception as e:
        return {
            "status": "unhealthy",
            "message": f"ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€: {str(e)[:100]}",
            "model": "none"
        }


# ------------------------------------------------------
# Embedding (ì˜µì…˜ 2)
# ------------------------------------------------------

class SummRequest(BaseModel):
    text: str


@app.post("/ai/summarize")
def summarize(req: SummRequest):
    text = req.text.strip()

    if not text:
        return {"summary": "ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}

    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (Gemini í† í° ì œí•œ ê³ ë ¤)
    if len(text) > 8000:
        text = text[:4000] + "\n...(ì¤‘ëµ)...\n" + text[-2000:]

    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ IT ê¸°ì—… ì±„ìš©ê³µê³  ì „ë¬¸ ìš”ì•½ê¸°ë‹¤.\n"
        "ê·œì¹™:\n"
        "1. ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ 5ê°œë¥¼ ì‘ì„±í•œë‹¤.\n"
        "2. ê° ë¬¸ì¥ì€ ë§ˆì¹¨í‘œ(.)ë¡œ ëë‚˜ì•¼ í•œë‹¤.\n"
        "3. ë¶ˆì™„ì „í•œ ë¬¸ì¥ì€ ì ˆëŒ€ ê¸ˆì§€ë‹¤.\n"
        "4. ë‹¤ìŒ ìˆœì„œë¡œ ì‘ì„±:\n"
        "   - 1ë¬¸ì¥: íšŒì‚¬ ì†Œê°œ ë˜ëŠ” ì„œë¹„ìŠ¤ ì„¤ëª…\n"
        "   - 2ë¬¸ì¥: ì£¼ìš” ì—…ë¬´ ë‚´ìš©\n"
        "   - 1ë¬¸ì¥: í•„ìˆ˜ ìê²©ìš”ê±´ (ê¸°ìˆ ìŠ¤íƒ í¬í•¨)\n"
        "   - 1ë¬¸ì¥: ìš°ëŒ€ì‚¬í•­ ë˜ëŠ” ë³µì§€\n"
        "5. ê° ë¬¸ì¥ì€ ìµœì†Œ 10ë‹¨ì–´ ì´ìƒì´ì–´ì•¼ í•œë‹¤.\n"
        "6. 'ë¦¬ë·°', 'ë¶„ì„', 'ì¡°ì–¸' ê°™ì€ ë©”íƒ€ ì–¸ê¸‰ ê¸ˆì§€.\n"
        "7. ì •ì§í•˜ê³  ê°„ê²°í•œ ë¬¸ì–´ì²´ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.\n"
    )

    user_prompt = (
        f"ë‹¤ìŒ ì±„ìš©ê³µê³ ë¥¼ ì •í™•íˆ 5ê°œì˜ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n"
        f"ê° ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.\n\n"
        f"ì±„ìš©ê³µê³  ë‚´ìš©:\n{text}\n\n"
        f"5ê°œ ë¬¸ì¥ ìš”ì•½ (ê° ë¬¸ì¥ì€ ë§ˆì¹¨í‘œë¡œ ëë‚¨):"
    )

    # 1ì°¨ ì‹œë„
    out = generate_text(
        sys_prompt,
        user_prompt,
        max_tokens=800,  # í† í° ìˆ˜ ì¦ê°€
        temperature=0.3,
    )

    # ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° 2ì°¨ ì‹œë„
    if not out or len(out.strip()) < 100:
        print(f"âš ï¸ 1ì°¨ ìš”ì•½ ì‹¤íŒ¨, 2ì°¨ ì‹œë„ ì¤‘... (ê¸¸ì´: {len(out) if out else 0})")

        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
        simple_prompt = (
            f"ë‹¤ìŒ ì±„ìš©ê³µê³ ë¥¼ 5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n\n{text[:3000]}\n\n"
            f"ìš”ì•½ (5ë¬¸ì¥):"
        )

        out = generate_text(
            "ë„ˆëŠ” ì±„ìš©ê³µê³  ìš”ì•½ ì „ë¬¸ê°€ë‹¤. í•­ìƒ 5ê°œì˜ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œë‹¤.",
            simple_prompt,
            max_tokens=800,
            temperature=0.4,
        )

    # ì—¬ì „íˆ ì‹¤íŒ¨í•œ ê²½ìš°
    if not out or len(out.strip()) < 50:
        return {"summary": "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}

    # ë¬¸ì¥ ê²€ì¦
    lines = [l.strip() for l in out.split("\n") if l.strip() and not l.strip().startswith("(")]
    sentences = []

    for line in lines:
        # ë§ˆì¹¨í‘œë¡œ ë¬¸ì¥ ë¶„ë¦¬
        parts = [s.strip() + "." for s in line.split(".") if s.strip() and len(s.strip()) > 5]
        sentences.extend(parts)

    # ìµœì†Œ 3ë¬¸ì¥ ì´ìƒ í™•ë³´
    if len(sentences) < 3:
        print(f"âš ï¸ ë¬¸ì¥ ìˆ˜ ë¶€ì¡±: {len(sentences)}ê°œ")
        # ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìµœì†Œí•œì˜ ë‚´ìš©ì´ë¼ë„ ì €ì¥)
        return {"summary": out.strip()}

    # ìƒìœ„ 5ë¬¸ì¥ë§Œ ì„ íƒ
    final_summary = " ".join(sentences[:5])

    return {"summary": final_summary}


@app.post("/ai/embed")
def embed(req: EmbedRequest):
    text = req.text.strip()
    if not text:
        return {"vector": []}

    try:
        emb = genai.embed_content(
            model="models/text-embedding-004",
            content=text
        )
        return {"vector": emb["embedding"]}
    except Exception as e:
        print("[EMBED ERROR]", e)
        return {"vector": []}


# ------------------------------------------------------
# match-one (ì˜µì…˜ 4)
# ------------------------------------------------------

@app.post("/ai/match-one")
def match_one(req: MatchOneRequest):
    """
    ë¬´ë£Œ í‹°ì–´ ìµœì í™” ë²„ì „
    - gemini-2.0-flash ì‚¬ìš©
    - rate limit: ë¶„ë‹¹ 15íšŒ, ì¼ì¼ 1,500íšŒ
    """
    resume = req.resume or ""
    job = req.job or ""

    if not resume.strip() or not job.strip():
        return {"score": 0, "reason": "ì…ë ¥ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}

    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œìœ¼ë¡œ í† í° ì ˆì•½
    resume = resume[:1000]
    job = job[:1000]

    try:
        # âœ… ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš©
        model = genai.GenerativeModel(
            model_name="gemini-2.0-flash",
            system_instruction="""You are a Korean job matching AI.
Evaluate resume-job match and respond with score (0-100) and Korean reason.

Scoring guide:
90-100: Perfect match
70-89: Good match
50-69: Fair match
30-49: Poor match
0-29: No match""",
            generation_config=genai.types.GenerationConfig(
                temperature=0.1,
                max_output_tokens=200,  # í† í° ì ˆì•½
                response_mime_type="application/json",
                response_schema={
                    "type": "object",
                    "properties": {
                        "score": {"type": "integer"},
                        "reason": {"type": "string"}
                    },
                    "required": ["score", "reason"]
                }
            )
        )

        prompt = f"""Resume: {resume}

Job: {job}

Evaluate match."""

        response = model.generate_content(prompt)
        result = json.loads(response.text)

        score = int(min(max(result["score"], 0), 100))
        reason = result["reason"]

        return {"score": score, "reason": reason}

    except Exception as e:
        print(f"âŒ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
        # í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±
        return keyword_based_match(resume, job)


def keyword_based_match(resume: str, job: str):
    """API ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±"""
    resume_lower = resume.lower()
    job_lower = job.lower()

    tech_keywords = {
        "backend": ["python", "java", "spring", "django", "node", "go", "kotlin"],
        "frontend": ["react", "vue", "angular", "javascript", "typescript", "css"],
        "database": ["mysql", "postgresql", "mongodb", "redis", "oracle"],
        "devops": ["aws", "docker", "kubernetes", "jenkins", "git", "linux"],
        "mobile": ["android", "ios", "swift", "flutter", "react native"]
    }

    total_matches = 0
    for keywords in tech_keywords.values():
        matches = sum(1 for kw in keywords if kw in resume_lower and kw in job_lower)
        total_matches += matches

    final_score = min(total_matches * 7, 70)
    reason = f"í‚¤ì›Œë“œ ë§¤ì¹­ ({total_matches}ê°œ ì¼ì¹˜)"

    return {"score": final_score, "reason": reason}


# ------------------------------------------------------
# ì¸í„°ë·° ì§ˆë¬¸ ìƒì„± API
# ------------------------------------------------------

class GenerateRequest(BaseModel):
    resumeId: int
    jobPostId: Optional[int] = None
    companyId: Optional[int] = None
    jobPostLink: Optional[str] = None
    companyLink: Optional[str] = None
    previousQuestions: List[str] = []

def fetch_url_content(url: str) -> str:
    """URLì—ì„œ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ (ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘)"""
    try:
        if not url.startswith("http"):
            return ""

        # í—¤ë” ì¶”ê°€ (ë´‡ ì°¨ë‹¨ ë°©ì§€)
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        resp = requests.get(url, headers=headers, timeout=5)
        if resp.status_code == 200:
            # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ì •ê·œì‹)
            text = re.sub(r'<[^>]+>', ' ', resp.text)
            # ê³µë°± ì •ë¦¬
            text = re.sub(r'\s+', ' ', text).strip()
            return text[:2000] # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
    except Exception as e:
        print(f"URL fetch error: {e}")
    return ""

@app.post("/interview/generate-questions")
def generate_questions(req: GenerateRequest):

    # === 1) Spring Bootë¡œë¶€í„° ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ===
    resume_text = ""
    job_text = ""
    company_text = ""

    # âš ï¸ ì—”ë“œí¬ì¸íŠ¸ ì£¼ì†ŒëŠ” ì‹¤ì œ ë„¤ í”„ë¡œì íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì • í•„ìš”
    BASE = "http://localhost:8080/api"

    try:
        resume_res = requests.get(f"{BASE}/resume/{req.resumeId}")
        if resume_res.status_code == 200:
            resume_data = resume_res.json()
            resume_text = resume_data.get("content", "")
    except:
        pass

    if req.jobPostId:
        try:
            job_res = requests.get(f"{BASE}/jobposts/{req.jobPostId}")
            if job_res.status_code == 200:
                job_data = job_res.json()
                job_text = job_data.get("content", "")
        except:
            pass

    if req.companyId:
        try:
            company_res = requests.get(f"{BASE}/companies/{req.companyId}")
            if company_res.status_code == 200:
                company_data = company_res.json()
                company_text = company_data.get("content", "")
        except:
            pass

    # === 1.5) ë§í¬ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (IDê°€ ì—†ê±°ë‚˜ ë§í¬ê°€ ìš°ì„ ì¼ ê²½ìš°) ===
    if not job_text and req.jobPostLink:
        job_text = fetch_url_content(req.jobPostLink)

    if not company_text and req.companyLink:
        company_text = fetch_url_content(req.companyLink)

    # === 2) Gemini í”„ë¡¬í”„íŠ¸ êµ¬ì„± ===
    system_prompt = """
ë„ˆëŠ” í•œêµ­ ITê¸°ì—… ì „ë¬¸ ë©´ì ‘ê´€ì´ë©°, ì‚¬ìš©ìì˜ ì´ë ¥ì„œÂ·ê³µê³ Â·ê¸°ì—… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 
ì‹¤ì œ ë©´ì ‘ì—ì„œ ë¬»ëŠ” ê³ ê¸‰ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•œë‹¤.

ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œë¼.

1) ì´ë¯¸ ì‚¬ìš©ëœ ì§ˆë¬¸(previousQuestions)ê³¼ **ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì§ˆë¬¸ë„ ì ˆëŒ€ ìƒì„± ê¸ˆì§€**
   - ë¬¸ì¥ë§Œ ë‹¤ë¥´ê³  ì˜ë¯¸ê°€ ê°™ìœ¼ë©´ ë¬´ì¡°ê±´ ì œì™¸
   - ì˜ˆ: â€œê¸°ìˆ ì  ë¬¸ì œë¥¼ í•´ê²°í•œ ê²½í—˜ì€?â€ vs â€œê¸°ìˆ ì ì¸ ë‚œê´€ì„ ê·¹ë³µí•œ ì‚¬ë¡€ëŠ”?â€
     â†’ ì„œë¡œ ìœ ì‚¬ ì§ˆë¬¸ìœ¼ë¡œ ì·¨ê¸‰

2) ì§ˆë¬¸ì€ ë°˜ë“œì‹œ â€œë‚´ìš©ì ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œâ€ì—¬ì•¼ í•œë‹¤.
   - ê¸°ìˆ , í˜‘ì—…, ë¬¸ì œ í•´ê²°, í”„ë¡œì íŠ¸ ë¦¬ë“œ, ì„±ê³¼ ë“± ì£¼ì œë¥¼ ë‹¤ì–‘í™”í•  ê²ƒ.

3) ê³µê³ ë‚˜ ê¸°ì—… ì •ë³´ê°€ ì—†ìœ¼ë©´ ì´ë ¥ì„œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
4) ê³µê³ /ê¸°ì—…/ì´ë ¥ì„œì—ì„œ ìœ ì¶” ê°€ëŠ¥í•œ í•µì‹¬ ì—­ëŸ‰ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
5) ë„ˆë¬´ í”í•œ ì§ˆë¬¸(ì˜ˆ: ìê¸°ì†Œê°œ, ì¥ë‹¨ì )ì€ ê¸ˆì§€

ì¶œë ¥ í˜•ì‹ì€ ë¬´ì¡°ê±´ ë‹¤ìŒ JSON ë°°ì—´ì´ì–´ì•¼ í•œë‹¤:
[
  {"id": 1, "question": "...", "category": "..."},
  ...
]
    """

    user_prompt = f"""
    [ì´ë ¥ì„œ ë‚´ìš©]
    {resume_text}

    [ê³µê³  ë‚´ìš©]
    {job_text or "ì—†ìŒ"}

    [ê¸°ì—… ì†Œê°œ]
    {company_text or "ì—†ìŒ"}

    [ì œì™¸í•  ì§ˆë¬¸ë“¤ (ì´ë¯¸ ì§ˆë¬¸í•¨)]
    {json.dumps(req.previousQuestions, ensure_ascii=False) if req.previousQuestions else "ì—†ìŒ"}

ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì£¼ì œê°€ ì„œë¡œ ë‹¤ë¥´ê³ ,
ì´ë¯¸ ì‚¬ìš©í•œ(previousQuestions) ì§ˆë¬¸ê³¼ ì˜ë¯¸ê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ”
ì‹ ê·œ ë©´ì ‘ ì§ˆë¬¸ 5ê°œë¥¼ ìƒì„±í•˜ë¼.

ìœ ì‚¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ë©´ ì•ˆ ëœë‹¤.
JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ë¼.
    """

    raw = call_llm_with_json(
        model="gemini-2.0-flash",
        system=system_prompt,
        prompt=user_prompt,
        max_tokens=600,
        temperature=0.5
    )

    questions = safe_json(raw)
    if not isinstance(questions, list):
        questions = []

    return questions




# ------------------------------------------------------
# ì¸í„°ë·° ë‹µë³€ í”¼ë“œë°± API
# ------------------------------------------------------

class FeedbackRequest(BaseModel):
    resumeId: Optional[int] = None
    jobPostLink: Optional[str] = None    # âœ… ì¶”ê°€
    companyLink: Optional[str] = None    # âœ… ì¶”ê°€
    question: str
    answer: str

class FeedbackResponse(BaseModel):
    feedback: str


@app.post("/interview/feedback", response_model=FeedbackResponse)
def interview_feedback(req: FeedbackRequest):
    """
    ë©´ì ‘ ë‹µë³€ í”¼ë“œë°± ìƒì„±
    """
    # === 1) Springì—ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì„ íƒì‚¬í•­) ===
    resume_text = ""
    job_text = ""
    company_text = ""

    BASE = "http://localhost:8080/api"

    # Resume ì •ë³´
    if req.resumeId:
        try:
            r = requests.get(f"{BASE}/resume/{req.resumeId}", timeout=5)
            if r.status_code == 200:
                resume_text = r.json().get("content", "")
        except:
            pass

    # ê³µê³  ë§í¬ëŠ” ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    if req.jobPostLink:
        job_text = f"ê³µê³  ë§í¬: {req.jobPostLink}"

    # ê¸°ì—… ë§í¬ëŠ” ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    if req.companyLink:
        company_text = f"ê¸°ì—… ë§í¬: {req.companyLink}"

    # === 2) Gemini í”„ë¡¬í”„íŠ¸ ===
    system_prompt = """ë„ˆëŠ” í•œêµ­ ITê¸°ì—…ì˜ ì‹¤ì œ ë©´ì ‘ê´€ì´ë‹¤.
ì‚¬ìš©ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³ , ë” ì¢‹ì€ ë‹µë³€ì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ì²¨ì‚­í•´ë¼.

ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë§ºì–´ë¼. ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ë¼.

ì¶œë ¥ í˜•ì‹:
### ë¬¸ì œì  ë¶„ì„
- ...

### ê°œì„  í¬ì¸íŠ¸
- ...

### ê°œì„ ëœ ì˜ˆì‹œ ë‹µë³€
...
"""

    user_prompt = f"""
[ì§ˆë¬¸]
{req.question}

[ì§€ì›ì ë‹µë³€]
{req.answer}

[ì°¸ê³  - ì´ë ¥ì„œ ìš”ì•½]
{resume_text[:500] if resume_text else "ì •ë³´ ì—†ìŒ"}

[ì°¸ê³  - ê³µê³  ë‚´ìš©]
{job_text[:500] if job_text else "ì •ë³´ ì—†ìŒ"}

[ì°¸ê³  - ê¸°ì—… ë‚´ìš©]
{company_text[:500] if company_text else "ì •ë³´ ì—†ìŒ"}

ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©´ì ‘ê´€ ì‹œê°ì—ì„œ ì²¨ì‚­í•´ë¼.
ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•´ë¼.
"""

    try:
        feedback = generate_text(
            system=system_prompt,
            prompt=user_prompt,
            max_tokens=2048,  # âœ… 1000 â†’ 2048ë¡œ ì¦ê°€
            temperature=0.3
        )

        # ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìœ¼ë©´ ê¸°ë³¸ í”¼ë“œë°±
        if not feedback or len(feedback.strip()) < 50:
            feedback = """### ğŸ“Š ë‹µë³€ ë¶„ì„

**ê°•ì :**
- ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì…¨ìŠµë‹ˆë‹¤.
- ë³¸ì¸ì˜ ìƒê°ì„ ëª…í™•íˆ ì „ë‹¬í•˜ì…¨ìŠµë‹ˆë‹¤.

**ê°œì„ ì :**
1. **STAR ê¸°ë²• í™œìš©**: ìƒí™©(Situation) - ê³¼ì œ(Task) - í–‰ë™(Action) - ê²°ê³¼(Result) êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. **êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¶”ê°€**: "ì„±ëŠ¥ ê°œì„ " â†’ "ì‘ë‹µì‹œê°„ 30% ê°œì„ "ì²˜ëŸ¼ ì •ëŸ‰ì  ì§€í‘œë¥¼ í¬í•¨í•˜ì„¸ìš”.
3. **ê¸°ìˆ  ìŠ¤íƒ ëª…ì‹œ**: ì‚¬ìš©í•œ ê¸°ìˆ ê³¼ ë„êµ¬ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš”.

**ì¶”ì²œ ë‹µë³€ ì‹œê°„:** 2-3ë¶„

**í•µì‹¬ í‚¤ì›Œë“œ:**
- ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
- ê¸°ìˆ ì  ì—­ëŸ‰
- í˜‘ì—… ê²½í—˜
- ì„±ê³¼ ì§€í‘œ
"""

        return {"feedback": feedback}

    except Exception as e:
        print(f"âŒ í”¼ë“œë°± ìƒì„± ì˜¤ë¥˜: {e}")
        return {
            "feedback": """### âš ï¸ í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨

AI í”¼ë“œë°± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.

**ê¸°ë³¸ ì¡°ì–¸:**
1. STAR ê¸°ë²•ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš” (ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼)
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš”
3. ê¸°ìˆ  ìŠ¤íƒì„ ëª…í™•íˆ ì–¸ê¸‰í•˜ì„¸ìš”
4. 2-3ë¶„ ë¶„ëŸ‰ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
5. í•µì‹¬ ì„±ê³¼ë¥¼ ê°•ì¡°í•˜ì„¸ìš”

ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."""
        }

# ===========================
# AI ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê³ ì •)
# ===========================


SITE_DOCUMENT = """
ğŸ“Œ ìš´ì˜ ëª©ì 
HireHubëŠ” ê°œë°œìÂ·IT ì§êµ° ì¤‘ì‹¬ì˜ ì±„ìš© í”Œë«í¼ìœ¼ë¡œì„œ, AIê°€ êµ¬ì§ì ë° ê¸°ì—… ì´ìš©ìì—ê²Œ ì •í™•í•˜ê³  ê²€ì¦ëœ ì •ë³´ë§Œ ì œê³µí•˜ë„ë¡ ì„¤ê³„ë˜ì–´ì•¼ í•œë‹¤.
AIëŠ” ì–´ë””ê¹Œì§€ë‚˜ ë³´ì¡° ìˆ˜ë‹¨ì´ë©°, ë²•ì Â·ì •ì±…ì  íŒë‹¨ì„ ëŒ€ì‹ í•˜ì§€ ì•ŠëŠ”ë‹¤.

-------------------------------------
1. ì„œë¹„ìŠ¤ ì†Œê°œ
-------------------------------------

HireHubëŠ” í•œêµ­ ê°œë°œìÂ·IT ì·¨ì—…ìë¥¼ ìœ„í•œ AI ê¸°ë°˜ ì±„ìš© í”Œë«í¼ì´ë‹¤.
í•µì‹¬ ê°€ì¹˜
êµ¬ì§ì: AI ê³µê³  ì¶”ì²œ Â· ë©´ì ‘ ì½”ì¹­ Â· ì´ë ¥ì„œ ë¶„ì„
ê¸°ì—…: ê³µê³  ê´€ë¦¬ Â· ê¸°ì—… í˜ì´ì§€ ì œê³µ
ëˆ„êµ¬ë‚˜ IT ì·¨ì—… ì ‘ê·¼ì„±ì„ ë†’ì´ëŠ” í”Œë«í¼

-------------------------------------
2. ì£¼ìš” ê¸°ëŠ¥
-------------------------------------
â‘  ê³µê³  ì¶”ì²œ
ë¡œê·¸ì¸ í•„ìš”
ì‚¬ìš©ì ê¸°ìˆ ìŠ¤íƒ, ê²½ë ¥, ì´ë ¥ì„œ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ëª¨ë¸
ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ë­í‚¹
ë¹„ë¡œê·¸ì¸: ì¡°íšŒìˆ˜ ê¸°ë°˜ â€œëª¨ë‘ê°€ ë³¸ ê³µê³ â€

â‘¡ ê³µê³  ì¡°íšŒ
ì¡°íšŒìˆ˜ìˆœ / ìµœì‹ ìˆœ / íšŒì‚¬ë³„ / ì¹´í…Œê³ ë¦¬ë³„
ìƒì„¸ í˜ì´ì§€ì—ì„œ íšŒì‚¬ ì†Œê°œ/ë³µì§€/ê¸°ìˆ ìŠ¤íƒ ì œê³µ
ìŠ¤í¬ë© ê°€ëŠ¥

â‘¢ ì´ë ¥ì„œ ê¸°ëŠ¥
ì—¬ëŸ¬ ê°œ ë“±ë¡ ê°€ëŠ¥
ì œì¶œ ì‹œ ì„ íƒ
AI ê¸°ë°˜ ì´ë ¥ì„œ ë¦¬ë·° ê°€ëŠ¥

â‘£ ì§€ì› ê¸°ëŠ¥
ê³µê³  ìƒì„¸ â†’ ì´ë ¥ì„œ ì„ íƒ â†’ ì œì¶œ
ë§ˆì´í˜ì´ì§€ì—ì„œ ì§€ì› ë‚´ì—­ í™•ì¸
ê¸°ì—… ì •ì±… ë”°ë¼ ì·¨ì†Œ ì—¬ë¶€ ë‹¬ë¼ì§

â‘¤ ê¸°ì—… ì •ë³´
íšŒì‚¬ ì†Œê°œ / ë³µì§€ / ì„¤ë¦½ë…„ë„ / ì£¼ì†Œ / ì±„ìš©ì¤‘ì¸ ê³µê³ 
ì¦ê²¨ì°¾ê¸° ê°€ëŠ¥

â‘¥ AI ê¸°ëŠ¥ (FastAPI ì—°ë™)
ì´ë ¥ì„œ ìš”ì•½
ê³µê³  ìš”ì•½
ì´ë ¥ì„œ ì²¨ì‚­
ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
ë©´ì ‘ ë‹µë³€ í”¼ë“œë°±
í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
ë¹„ì†ì–´ ê°ì§€ / moderation

â‘¦ ê³ ê°ì„¼í„° ì±—ë´‡
ê¸°ë³¸ ì‘ë‹µ: AI ìƒë‹´
WebSocket ê¸°ë°˜ ìƒë‹´ì‚¬ ì—°ê²° ê°€ëŠ¥
10ë¶„ ë¹„í™œì„± ì‹œ ìë™ ì¢…ë£Œ
ëŒ€í™” ë‚´ìš©ì€ localStorage ì €ì¥
DB ì €ì¥ì€ ì„ íƒ ê¸°ëŠ¥

-------------------------------------
3. ì¸ì¦ / íšŒì› / ì ‘ê·¼ ì œì•½
-------------------------------------
ë¡œê·¸ì¸ í•„ìš”í•œ ê¸°ëŠ¥
ê³µê³  ì¶”ì²œ
ì§€ì›í•˜ê¸°
ìƒë‹´ì‚¬ ì—°ê²°(í•¸ë“œì˜¤í”„)
ì´ë ¥ì„œ ê´€ë¦¬
AI ê³ ë„í™” ê¸°ëŠ¥ ëŒ€ë¶€ë¶„
ë¹„ë¡œê·¸ì¸ ê°€ëŠ¥
ê³µê³  ì¡°íšŒ
ê¸°ì—… ì¡°íšŒ
ê²Œì‹œíŒ ì—´ëŒ
ê¸°ë³¸ ì±—ë´‡
ì¸ì¦ ì •ì±…
ì´ë©”ì¼ íšŒì›ê°€ì…
JWT ê¸°ë°˜ ì¸ì¦
ê°œì¸ì •ë³´ëŠ” ìƒë‹´ì‚¬ë„ ì¡°íšŒ ë¶ˆê°€

-------------------------------------
4. ê²Œì‹œíŒ(ì»¤ë®¤ë‹ˆí‹°) ê·œì¹™
-------------------------------------
ê¸€ ì‘ì„±/ìˆ˜ì •/ì‚­ì œ ê°€ëŠ¥
í•„í„°ë§ ë‹¨ì–´ í¬í•¨ ì‹œ ì‘ì„± ì°¨ë‹¨
ë³¸ì¸ ê¸€ë§Œ ìˆ˜ì •Â·ì‚­ì œ ê°€ëŠ¥
ì¡°íšŒìˆ˜ ì¹´ìš´íŠ¸
ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ

-------------------------------------
5. ì •ì±… ë° ê¸ˆì§€ì‚¬í•­
-------------------------------------
ì ˆëŒ€ ì œê³µ ê¸ˆì§€
ë‚´ë¶€ ë°ì´í„°, ì§€ì›ì ìˆ˜, ê²½ìŸë¥  ì˜ˆì¸¡
ì‹¤ì œ í•©ê²© ê°€ëŠ¥ì„± ì˜ˆì¸¡
ê°œì¸ì •ë³´ ë…¸ì¶œ
ë‚´ë¶€ ì„œë²„ êµ¬ì¡° / DB ìŠ¤í‚¤ë§ˆ
íšŒì‚¬ ë‚´ë¶€ ê¸°ë°€
â€œí™•ì •ì  ì¡°ì–¸â€ (ë²•ë¥ /ì˜ë£Œ ë“±)
HireHub ìš´ì˜ ì •ì±… ë³€ê²½ ê°€ëŠ¥í•˜ë‹¤ê³  ì–¸ê¸‰ ê¸ˆì§€
í•­ìƒ ì§€ì¼œì•¼ í•  ì›ì¹™
ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€
ì‚¬ìš©ì ë¶ˆí¸ â†’ í•´ê²° ê²½ë¡œ ì œê³µ
ê¸°ëŠ¥ ì—†ìŒ â†’ "í˜„ì¬ ê¸°ëŠ¥ì—ì„œëŠ” ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
ë¯¼ê° ì •ë³´ ìš”ì²­ â†’ â€œì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤â€
ìœ ì € ê°ì • ì™„í™”, ì±…ì„ íšŒí”¼ ì—†ëŠ” ì •í™•í•œ ì•ˆë‚´

-------------------------------------
6. í† í° ì •ì±… (ë§¤ìš° ì¤‘ìš”)
-------------------------------------
HireHubì˜ AI ê¸°ëŠ¥ì€ í† í° ê¸°ë°˜ ìœ ë£Œ ê¸°ëŠ¥ì´ë‹¤.
ğŸ”¹ í† í° êµ¬ë§¤
10 Tokens = 1,000ì›
30 Tokens = 2,900ì›
50 Tokens = 4,800ì›
ê²°ì œ ìˆ˜ë‹¨: ì¹´ë“œ(ì´ë‹ˆì‹œìŠ¤), ì¹´ì¹´ì˜¤í˜ì´
ğŸ”¹ ê¸°ëŠ¥ë³„ í† í° ì°¨ê°ëŸ‰
ê¸°ëŠ¥ëª…	ì°¨ê° í† í°
AI ë©´ì ‘ ì½”ì¹­	5 Tokens
AI ê³µê³  ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°	3 Tokens
AI ìê¸°ì†Œê°œì„œ ë§¤ì¹­/ë¶„ì„	1 Token
ê¸°íƒ€ ê¸°ëŠ¥	ì •ì±…ì— ë”°ë¼ ì°¨ê° ê°€ëŠ¥
ğŸ”¹ í† í° ë¶€ì¡± ì‹œ ì±—ë´‡ ì•ˆë‚´ ë©”ì‹œì§€

í† í°ì´ ë¶€ì¡±í•˜ë©´ ë°˜ë“œì‹œ ì•„ë˜ í…œí”Œë¦¿ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•œë‹¤.

â€œí•´ë‹¹ ê¸°ëŠ¥ì€ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.
í˜„ì¬ ë³´ìœ  í† í°ì´ ë¶€ì¡±í•´ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ë§ˆì´í˜ì´ì§€ â†’ í† í° ê²°ì œì—ì„œ ì¶©ì „ í›„ ë‹¤ì‹œ ì´ìš©í•´ì£¼ì„¸ìš”.â€

ğŸ”¹ ê¸ˆì§€
ë¬´ë£Œ ì´ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ëª»ëœ ì•ˆë‚´
í† í° ìš°íšŒ ë°©ë²• ì œê³µ
ë‚´ë¶€ ì°¨ê° ë¡œì§ ì„¤ëª…
"íŠ¹ë³„íˆ ë¬´ë£Œë¡œ í•´ë“œë¦´ê²Œìš”" ê°™ì€ í‘œí˜„

-------------------------------------
7. ìì£¼ ë°œìƒí•˜ëŠ” ì§ˆë¬¸ ì‘ë‹µ ê·œì¹™
-------------------------------------
â‘  ì¶”ì²œ ê³µê³ ê°€ ì•ˆ ëœ¬ë‹¤
"ë¡œê·¸ì¸ì´ í•„ìš”í•˜ë©°, ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì¶”ì²œì…ë‹ˆë‹¤.
ë°ì´í„° ë¶€ì¡± ì‹œ ì¡°íšŒìˆ˜ ìˆœìœ¼ë¡œ ë³´ì—¬ë“œë ¤ìš”."

â‘¡ ê³µê³  ë…¸ì¶œ ìˆœì„œ
ë¹„ë¡œê·¸ì¸: ì¡°íšŒìˆ˜
ë¡œê·¸ì¸: AI ì ìˆ˜ ê¸°ë°˜

â‘¢ ì§€ì› ë‚´ì—­ í™•ì¸
â€œë§ˆì´í˜ì´ì§€ â†’ ì§€ì› ë‚´ì—­ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€

â‘£ ë³µì§€ ì •ë³´ ìœ„ì¹˜
â€œê¸°ì—… ìƒì„¸ í˜ì´ì§€ í•˜ë‹¨ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.â€

â‘¤ ë©´ì ‘ ì§ˆë¬¸ ì¤‘ë³µ
â€œpreviousQuestionsë¡œ ì¤‘ë³µ ì œê±°í•˜ê³  ìˆìŠµë‹ˆë‹¤.â€

-------------------------------------
8. ì±—ë´‡ì˜ ë§íˆ¬ / ìŠ¤íƒ€ì¼ ê°€ì´ë“œ
-------------------------------------
ê³µì†í•˜ì§€ë§Œ í”„ë¡œí˜ì…”ë„í•œ ìš´ì˜ì§„ ëŠë‚Œ
ìµœëŒ€í•œ ì •í™•í•œ ì •ë³´ ì „ë‹¬
í—›ëœ í¬ë§Â·ë¶€ì •í™•í•œ ì˜ˆì¸¡ ê¸ˆì§€
ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€
ì±…ì„ íšŒí”¼í˜• í‘œí˜„ ê¸ˆì§€
ë¶ˆí¸ â†’ í•´ê²°ì±… ì œì‹œ
ìš•ì„¤Â·ë¹„ì†ì–´ì—ëŠ” ì°¨ë¶„í•˜ê²Œ ê²½ê³ 

-------------------------------------
9. ì±—ë´‡ì˜ AI ê¸°ëŠ¥ ì‹¤í–‰ ê·œì¹™
-------------------------------------
ê¸°ëŠ¥ ì‹¤í–‰ ì „ ì²´í¬
ë¡œê·¸ì¸ í•„ìš” ì—¬ë¶€ í™•ì¸
í† í° ë³´ìœ ëŸ‰ í™•ì¸
ë¶€ì¡±í•˜ë©´ í† í° ê²°ì œ ì•ˆë‚´
AI ê¸°ëŠ¥ í˜¸ì¶œ
ë°ì´í„° ì ‘ê·¼ ê·œì¹™
ì‹¤ì œ DB ìœ ì € ì •ë³´ ì¡°íšŒ ë¶ˆê°€
â€œí™•ì¸ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤â€ë¡œ ì‘ë‹µ
ê²Œì‹œë¬¼, ì´ë ¥ì„œ ë“±ì€ ì‚¬ìš©ìê°€ ì œê³µí•œ ë‚´ìš©ë§Œ í™œìš©

-------------------------------------
10. ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ
-------------------------------------
â€œì§€ì›ë¥  ë†’ì•„ìš” / í•©ê²© ê°€ëŠ¥ì„± ë†’ì•„ìš”â€
â€œì´ íšŒì‚¬ ê²½ìŸë¥ ì€ ~ëª…ì…ë‹ˆë‹¤â€
â€œì„œë²„ êµ¬ì¡°ëŠ” ~ì…ë‹ˆë‹¤â€
â€œí† í° ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤â€
â€œì •ì±… ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤â€
íŠ¹ì • ê¸°ì—…ì— ëŒ€í•œ ë£¨ë¨¸ ì œê³µ
ì •ì¹˜ì Â·ë²•ì Â·ì˜ë£Œì  ì¡°ì–¸

-------------------------------------
11. ìµœì¢… ì‘ë‹µ ìŠ¤íƒ€ì¼ ìš”ì•½
-------------------------------------
ì •í™•Â·ëª…í™•Â·ê³¼ë„í•œ ì¹œì ˆ ì—†ìŒ
ìš´ì˜ ê°€ì´ë“œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
ëª¨ë¥´ë©´ ë‹¨ì •í•˜ì§€ ì•Šê³  ì•Œë ¤ì§„ ë²”ìœ„ì—ì„œë§Œ ë§í•˜ê¸°
ê¸°ëŠ¥ ì•ˆë‚´ëŠ” ë‹¨ê³„ì ìœ¼ë¡œ
íŒë‹¨ ëŒ€ì‹  ì„ íƒì§€ ì œì‹œ

ì´ ëª¨ë“  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆì˜ì— ë‹µë³€í•œë‹¤."ë¼ê³  ë§í•œë‹¤.
"""

BASE_SYSTEM_PROMPT = f"""
ë„ˆëŠ” HireHub(noeyos.store)ì˜ ê³µì‹ AI ê³ ê°ì„¼í„° ì±—ë´‡ì´ë‹¤.
ì•„ë˜ ì‚¬ì´íŠ¸ ì„¤ëª…ì€ ë„ˆì˜ â€œì§€ì‹ë² ì´ìŠ¤â€ì´ë©°, ëª¨ë“  ë‹µë³€ì€ ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ í•´ì•¼ í•œë‹¤.
ì§€ì‹ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ë§ê³  â€œí˜„ì¬ ê¸°ëŠ¥ì—ì„œëŠ” ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.â€ë¼ê³  ë‹µë³€í•œë‹¤.

=== ì‚¬ì´íŠ¸ ì„¤ëª… ì‹œì‘ ===
{SITE_DOCUMENT}
=== ì‚¬ì´íŠ¸ ì„¤ëª… ë ===

ë‹µë³€ ê·œì¹™:
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ìµœëŒ€í•œ ê°„ë‹¨í•˜ê³  ì •í™•í•˜ê²Œ í•´ê²°í•œë‹¤.
2. ì‹¤ì œ HireHub ì„œë¹„ìŠ¤ íë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
3. ì •ì±…Â·ê¸°ëŠ¥ì´ ì—†ëŠ” ê²½ìš° ì—†ëŠ” ëŒ€ë¡œ ì •í™•í•˜ê²Œ ì•ˆë‚´í•œë‹¤.
4. ìœ ì €ê°€ ì–´ë ¤ì›Œí•˜ë©´ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•œë‹¤.
5. ê³ ê°ì„¼í„° ì§ì›ì²˜ëŸ¼ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•œë‹¤.
6. ì±„ìš©, ë²•ë¥ , ì˜í•™ì  ì¡°ì–¸ì€ ì œê³µí•˜ì§€ ì•ŠëŠ”ë‹¤.
"""

# ------------------------------------------------------
# AI ChatBot (Gemini ê¸°ë°˜)
# ------------------------------------------------------

class ChatRequest(BaseModel):
    userId: int
    sessionId: str
    message: str

@app.post("/ai/chat")
async def chat(req: ChatRequest):
    try:
        print(f"ğŸ“¨ ë°›ì€ ë©”ì‹œì§€: {req.message}")

        answer = generate_text(
            system=BASE_SYSTEM_PROMPT,    # <-- ì‚¬ì´íŠ¸ ì„¤ëª… ì ìš©ë¨
            prompt=req.message,
            max_tokens=600,
            temperature=0.5
        )

        # â­ AI ê¸°ë¡ ì €ì¥: Spring ë¡œ ì „ì†¡
        save_chat_to_spring(
            user_id=req.userId,
            session_id=req.sessionId,
            user_message=req.message,
            bot_answer=answer
        )

        return {"answer": answer}

    except Exception as e:
        return {"answer": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

# ------------------------------------------------------
# ìƒíƒœ & ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
# ------------------------------------------------------

@app.get("/health")
def health():
    return {"ok": True}

@app.get("/ai/models")
def list_models():
    try:
        return [{"name": m.name} for m in genai.list_models()]
    except Exception as e:
        return {"error": str(e)}


# Rate limiter ë°ì½”ë ˆì´í„°
def rate_limit(calls_per_minute=15):
    """
    Gemini APIì˜ ê¸°ë³¸ rate limitì€ ë¶„ë‹¹ 15íšŒ
    """
    min_interval = 60.0 / calls_per_minute
    last_called = [0.0]

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed

            if left_to_wait > 0:
                time.sleep(left_to_wait)

            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret

        return wrapper

    return decorator


# generate_text í•¨ìˆ˜ì— ì ìš©
@rate_limit(calls_per_minute=15)  # ë¶„ë‹¹ 15íšŒë¡œ ì œí•œ
def generate_text_safe(system, prompt, max_tokens=512, temperature=0.3):
    """Rate limitì´ ì ìš©ëœ ì•ˆì „í•œ ë²„ì „"""
    return generate_text(system, prompt, max_tokens, temperature)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)
